{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sdm/Desktop/shakib/KUET LLM/data/rag4.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag2V2.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag1.docx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = r'/home/sdm/Desktop/shakib/KUET LLM/data/'\n",
    "file_pattern = '*.docx'  \n",
    "file_paths = glob.glob(directory_path + file_pattern)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e58ae28b8fb41baa32486a563c520c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "# path='google/gemma-7b'\n",
    "path=\"KUETLLM_zepyr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path,\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "\n",
    ")\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    \n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vectorstore(flag):\n",
    "    if flag == False:\n",
    "        flag=True\n",
    "        # loader = Docx2txtLoader(r\"data/Khulna University of Engineering.docx\")\n",
    "        # documents=loader.load()\n",
    "        all_doc=[]\n",
    "        for x in file_paths:\n",
    "            loader = Docx2txtLoader(x)\n",
    "            documents=loader.load()\n",
    "            all_doc.extend(documents)\n",
    "        # text_splitter=splitter_ret()\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=500,chunk_overlap=30,separator=\"\\n\")\n",
    "        docs=text_splitter.split_documents(documents=all_doc)\n",
    "        # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        # docs = text_splitter.split_documents(documents)\n",
    "        model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        hf = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        vectorstore=FAISS.from_documents(docs,hf)\n",
    "        vectorstore.save_local('vectorstore')\n",
    "        return vectorstore\n",
    "    else:\n",
    "        model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        hf = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        new_vectorstore=FAISS.load_local(\"vectorstore\",hf)\n",
    "        return new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template='''\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context and history to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "history: {history} \n",
    "Context: {context} \n",
    "Answer:\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "retriever = create_vectorstore(True).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of documents to consider\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Threshold for document relevance\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummarization_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextractive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Method for summarizing documents\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Language of the documents\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m question_generator_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     11\u001b[0m chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain(\n\u001b[1;32m     12\u001b[0m     combine_docs_chain\u001b[38;5;241m=\u001b[39mcombine_docs_chain,\n\u001b[1;32m     13\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m     14\u001b[0m     question_generator\u001b[38;5;241m=\u001b[39mquestion_generator_chain,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmtesting/lib/python3.9/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llmtesting/lib/python3.9/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/anaconda3/envs/llmtesting/lib/python3.9/site-packages/pydantic/v1/main.py:1048\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m__pre_root_validators__:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1048\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28mset\u001b[39m(), ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], cls_)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmtesting/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:158\u001b[0m, in \u001b[0;36mStuffDocumentsChain.get_default_document_variable_name\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;129m@root_validator\u001b[39m(pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_document_variable_name\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get default document variable name, if not provided.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    If only one variable is present in the llm_chain.prompt,\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    we can infer that the formatted documents should be passed in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    with this variable name.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     llm_chain_variables \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_chain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_variable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(llm_chain_variables) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'llm_chain'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "combine_docs_chain = StuffDocumentsChain(\n",
    "    num_docs=5,  # Number of documents to consider\n",
    "    similarity_threshold=0.7,  # Threshold for document relevance\n",
    "    summarization_method='extractive',  # Method for summarizing documents\n",
    "    language='en'  # Language of the documents\n",
    ")\n",
    "question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain = ConversationalRetrievalChain(\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "questions=[\n",
    "    'How many halls in KUET?', 'weather of KUET',\"What is the most beautiful things in KUET\", \"IS KUET RAGGING FREE?\",\"Why KUET CSE is called CSE Family? \"\n",
    "]\n",
    "demo = gr.ChatInterface(fn=ans_ret, examples=questions, title=\"KUET information Bot\")\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
