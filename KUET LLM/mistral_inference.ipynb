{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import transformers\n",
    "import torch\n",
    "# base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "# full_output = 'models/full_KUET_LLM_Mistral'\n",
    "full_output = 'models/full_KUET_LLM_zepyhr'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
    "# pipe = pipeline(task=\"text-generation\", model=full_output, tokenizer=tokenizer, max_length=630)\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=full_output,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "from langchain import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "    <s>[INST] <<SYS>>\n",
    "    {role}\n",
    "    <</SYS>>       \n",
    "    {text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\n",
    "        \"role\", \n",
    "        \"text\"\n",
    "    ],\n",
    "    template = template,\n",
    ")\n",
    "\n",
    "role = \"You are a KUET authority managed chatbot, help users by answering their queries about KUET.\"\n",
    "\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dat=[]\n",
    "cnt=0\n",
    "df=pd.read_excel(r\"data/KUET information2k20.xlsx\")\n",
    "\n",
    "for id,ques in tqdm.tqdm(zip(df['id'],df['Question'])):\n",
    "    ans=chain.invoke({\"role\": role,\"text\":ques})\n",
    "    dat.append({\n",
    "        \"id\":id,\n",
    "        \"question\":ques,\n",
    "        \"answer\":ans\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa=pd.DataFrame(dat)\n",
    "model_ans=\"model_ans_zepyhr_finetuned486_without_rag\"\n",
    "sa.to_excel(f\"data//{model_ans}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output = 'models/full_KUET_LLM_llama'\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_output)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"shahidul034/KUETLLM_llama2\"\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
