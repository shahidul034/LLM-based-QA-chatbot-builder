{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sdm/Desktop/shakib/KUET LLM/data/rag4.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag2V2.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag1.docx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = r'/home/sdm/Desktop/shakib/KUET LLM/data/'\n",
    "file_pattern = '*.docx'  \n",
    "file_paths = glob.glob(directory_path + file_pattern)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter_ret():\n",
    "    length_function = len\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=100,\n",
    "    length_function=length_function,\n",
    ")\n",
    "    return splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vectorstore(flag):\n",
    "    if flag == False:\n",
    "        flag=True\n",
    "        # loader = Docx2txtLoader(r\"data/Khulna University of Engineering.docx\")\n",
    "        # documents=loader.load()\n",
    "        all_doc=[]\n",
    "        for x in file_paths:\n",
    "            loader = Docx2txtLoader(x)\n",
    "            documents=loader.load()\n",
    "            all_doc.extend(documents)\n",
    "        # text_splitter=splitter_ret()\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=500,chunk_overlap=30,separator=\"\\n\")\n",
    "        docs=text_splitter.split_documents(documents=all_doc)\n",
    "        # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        # docs = text_splitter.split_documents(documents)\n",
    "        model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        hf = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        vectorstore=FAISS.from_documents(docs,hf)\n",
    "        vectorstore.save_local('vectorstore')\n",
    "        return vectorstore\n",
    "    else:\n",
    "        model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        hf = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        new_vectorstore=FAISS.load_local(\"vectorstore\",hf)\n",
    "        return new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 854, which is longer than the specified 500\n",
      "Created a chunk of size 576, which is longer than the specified 500\n",
      "Created a chunk of size 723, which is longer than the specified 500\n",
      "Created a chunk of size 751, which is longer than the specified 500\n",
      "Created a chunk of size 570, which is longer than the specified 500\n",
      "Created a chunk of size 737, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 632, which is longer than the specified 500\n",
      "Created a chunk of size 585, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 922, which is longer than the specified 500\n",
      "Created a chunk of size 595, which is longer than the specified 500\n",
      "Created a chunk of size 587, which is longer than the specified 500\n",
      "Created a chunk of size 626, which is longer than the specified 500\n",
      "Created a chunk of size 523, which is longer than the specified 500\n",
      "Created a chunk of size 736, which is longer than the specified 500\n",
      "Created a chunk of size 558, which is longer than the specified 500\n",
      "Created a chunk of size 735, which is longer than the specified 500\n",
      "Created a chunk of size 955, which is longer than the specified 500\n",
      "Created a chunk of size 615, which is longer than the specified 500\n",
      "Created a chunk of size 914, which is longer than the specified 500\n",
      "Created a chunk of size 611, which is longer than the specified 500\n",
      "Created a chunk of size 804, which is longer than the specified 500\n",
      "Created a chunk of size 647, which is longer than the specified 500\n",
      "Created a chunk of size 548, which is longer than the specified 500\n",
      "Created a chunk of size 1388, which is longer than the specified 500\n",
      "Created a chunk of size 1025, which is longer than the specified 500\n",
      "Created a chunk of size 861, which is longer than the specified 500\n",
      "Created a chunk of size 868, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 631, which is longer than the specified 500\n",
      "Created a chunk of size 895, which is longer than the specified 500\n",
      "Created a chunk of size 578, which is longer than the specified 500\n",
      "Created a chunk of size 980, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 503, which is longer than the specified 500\n",
      "Created a chunk of size 833, which is longer than the specified 500\n",
      "Created a chunk of size 529, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 674, which is longer than the specified 500\n",
      "Created a chunk of size 748, which is longer than the specified 500\n",
      "Created a chunk of size 816, which is longer than the specified 500\n",
      "Created a chunk of size 605, which is longer than the specified 500\n",
      "Created a chunk of size 602, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 894, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 748, which is longer than the specified 500\n",
      "Created a chunk of size 520, which is longer than the specified 500\n",
      "Created a chunk of size 886, which is longer than the specified 500\n",
      "Created a chunk of size 1006, which is longer than the specified 500\n",
      "Created a chunk of size 1145, which is longer than the specified 500\n",
      "Created a chunk of size 1065, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 1456, which is longer than the specified 500\n",
      "Created a chunk of size 662, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 659, which is longer than the specified 500\n",
      "Created a chunk of size 529, which is longer than the specified 500\n",
      "Created a chunk of size 628, which is longer than the specified 500\n",
      "Created a chunk of size 1517, which is longer than the specified 500\n",
      "Created a chunk of size 629, which is longer than the specified 500\n",
      "Created a chunk of size 666, which is longer than the specified 500\n",
      "Created a chunk of size 1269, which is longer than the specified 500\n",
      "Created a chunk of size 1100, which is longer than the specified 500\n",
      "Created a chunk of size 556, which is longer than the specified 500\n",
      "Created a chunk of size 604, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 710, which is longer than the specified 500\n",
      "Created a chunk of size 699, which is longer than the specified 500\n",
      "Created a chunk of size 720, which is longer than the specified 500\n",
      "Created a chunk of size 672, which is longer than the specified 500\n",
      "Created a chunk of size 731, which is longer than the specified 500\n",
      "Created a chunk of size 1487, which is longer than the specified 500\n",
      "Created a chunk of size 1230, which is longer than the specified 500\n",
      "Created a chunk of size 651, which is longer than the specified 500\n",
      "Created a chunk of size 639, which is longer than the specified 500\n",
      "Created a chunk of size 631, which is longer than the specified 500\n",
      "Created a chunk of size 875, which is longer than the specified 500\n",
      "Created a chunk of size 640, which is longer than the specified 500\n",
      "Created a chunk of size 1095, which is longer than the specified 500\n",
      "Created a chunk of size 1015, which is longer than the specified 500\n",
      "Created a chunk of size 808, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 531, which is longer than the specified 500\n",
      "Created a chunk of size 558, which is longer than the specified 500\n",
      "Created a chunk of size 511, which is longer than the specified 500\n",
      "Created a chunk of size 551, which is longer than the specified 500\n",
      "Created a chunk of size 628, which is longer than the specified 500\n",
      "Created a chunk of size 705, which is longer than the specified 500\n",
      "Created a chunk of size 706, which is longer than the specified 500\n",
      "Created a chunk of size 956, which is longer than the specified 500\n",
      "Created a chunk of size 787, which is longer than the specified 500\n",
      "Created a chunk of size 652, which is longer than the specified 500\n",
      "Created a chunk of size 695, which is longer than the specified 500\n",
      "Created a chunk of size 746, which is longer than the specified 500\n",
      "Created a chunk of size 762, which is longer than the specified 500\n",
      "Created a chunk of size 1817, which is longer than the specified 500\n",
      "Created a chunk of size 603, which is longer than the specified 500\n",
      "Created a chunk of size 772, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 1044, which is longer than the specified 500\n",
      "Created a chunk of size 1320, which is longer than the specified 500\n",
      "Created a chunk of size 890, which is longer than the specified 500\n",
      "Created a chunk of size 501, which is longer than the specified 500\n",
      "Created a chunk of size 601, which is longer than the specified 500\n",
      "Created a chunk of size 1059, which is longer than the specified 500\n",
      "Created a chunk of size 1085, which is longer than the specified 500\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "vectorstore=create_vectorstore(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0f5f0735184ac58e3cd73f3da0611c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "# path='google/gemma-7b'\n",
    "path=\"KUETLLM_zepyr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path,\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(path,\n",
    "#                                              device_map='auto',\n",
    "#                                              torch_dtype=torch.float16,\n",
    "#                                              use_auth_token=True,\n",
    "#                                              load_in_8bit=True,\n",
    "#                                             #  load_in_4bit=True\n",
    "#                                              )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "\n",
    ")\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    \n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_ret(inp,history):\n",
    "    ans=rag_chain.invoke(inp)\n",
    "    k=ans.split(\"Based on the text material\")\n",
    "    k2=ans.split(\"Hope that helped! Let me know if you have any more questions.\")\n",
    "\n",
    "    if len(k)>=2:\n",
    "        k3=k[0].split(\"Hope that helped! Let me know if you have any more questions.\")\n",
    "        if len(k3)>=2:\n",
    "            return k3[0]\n",
    "        else:\n",
    "            return k[0]\n",
    "    if len(k2)>=2:\n",
    "        return k2[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://be9d598fb7ad66b31e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://be9d598fb7ad66b31e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "questions=[\n",
    "    'How many halls in KUET?', 'weather of KUET',\"What is the most beautiful things in KUET\", \"IS KUET RAGGING FREE?\",\"Why KUET CSE is called CSE Family? \"\n",
    "]\n",
    "demo = gr.ChatInterface(fn=ans_ret, examples=questions, title=\"KUET information Bot\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
