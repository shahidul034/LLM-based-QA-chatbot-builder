{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "lora_output = 'models/KUETLLM_zephyr7b_lora'\n",
    "full_output = 'models/KUETLLM_zephyr7b_beta'\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(\"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\") #arbit\n",
    "# login(\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### huggingface dataset with Prompt, Answer pair\n",
    "\n",
    "# data = load_dataset(\"huggingface/repo\", split=\"train\")\n",
    "# data_df = data.to_pandas()\n",
    "\n",
    "### read csv with Prompt, Answer pair \n",
    "data_location = r\"data/dataset_shakibV1.xlsx\"\n",
    "# data_df=pd.read_csv( data_location ,encoding='unicode_escape')\n",
    "data_df=pd.read_excel( data_location)\n",
    "\n",
    "## formatting function using tokenizer chat template, system text is set for KUETLLM\n",
    "def formatted_text(x):\n",
    "    temp = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a KUET authority managed chatbot, help users by answering their queries about KUET.\"},\n",
    "    {\"role\": \"user\", \"content\": x[\"Prompt\"]},\n",
    "    {\"role\": \"assistant\", \"content\": x[\"Reply\"]}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(temp, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "\n",
    "## set formatting\n",
    "data_df[\"text\"] = data_df[[\"Prompt\", \"Reply\"]].apply(lambda x: formatted_text(x), axis=1)\n",
    "print(data_df.iloc[0])\n",
    "dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "\n",
    "\n",
    "# tokenized_texts = tokenizer(dataset['text'])\n",
    "\n",
    "\n",
    "# tokenized_text_lengths = [len(tokens) for tokens in tokenized_texts['input_ids']]\n",
    "\n",
    "\n",
    "# filtered_indices = [i for i, length in enumerate(tokenized_text_lengths) if length <= 200]\n",
    "\n",
    "\n",
    "# filtered_dataset = dataset.select(filtered_indices)\n",
    "\n",
    "\n",
    "# print(filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatted_text(x):\n",
    "#     return f'''<s>### Instruction\n",
    "#     Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "#     ### Input:\n",
    "#     {x[\"Prompt\"]}\n",
    "\n",
    "#     ### Response:\n",
    "#     {x[\"Reply\"]}</s>\n",
    "#     '''\n",
    "# data_df[\"text\"] = data_df[[\"Prompt\", \"Reply\"]].apply(lambda x: formatted_text(x), axis=1)\n",
    "# print(data_df.iloc[0])\n",
    "# dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# target modules are currently selected for zephyr base model\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],   # target all the linear layers for full finetuning\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)\n",
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=4\n",
    "GRAD_ACC=4\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=5e-06                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=10,\n",
    "                                                 num_train_epochs = 2,\n",
    "                                                 output_dir=lora_output,\n",
    "                                                 remove_unused_columns=False,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False, )\n",
    "\n",
    "\n",
    "# Setup trainer\n",
    "trainer = SFTTrainer(model=model,\n",
    "                               train_dataset=dataset,\n",
    "                               data_collator=data_collator,\n",
    "                              # tokenizer= tokenizer,\n",
    "                               args=training_config,\n",
    "                               dataset_text_field=\"text\",\n",
    "                               max_seq_length= MAXLEN\n",
    "                              \n",
    "                            #    callbacks=[early_stop], need to learn, lora easily overfits\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(\"zephyr-7b-beta-base-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, lora_output)\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config.base_model_name_or_path,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(full_output)\n",
    "tokenizer.save_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push model to hub\n",
    "# merged_model.push_to_hub(full_output)\n",
    "# tokenizer.push_to_hub(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load for inferencing\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_output)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"shahidul034/KUETLLM_zephyr_base\")\n",
    "# tokenizer.push_to_hub(\"shahidul034/KUETLLM_zephyr_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# model_name = r\"/home/rtx3090/Desktop/shakib/sql/sqlcoder/model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_output)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    full_output,\n",
    "    trust_remote_code=True,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load base for comparison\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"zephyr7b-beta-full\")\n",
    "# tokenizer.save_pretrained(\"zephyr7b-beta-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp=f'''<s>### Instruction\n",
    "    Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "    ### Input:\n",
    "    Can you describe the location of Khulna City?\n",
    "\n",
    "    ### Response:\n",
    "    </s>\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generate(inp,history):    \n",
    "    inputs = tokenizer(inp, return_tensors=\"pt\")\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    import time\n",
    "    st_time = time.time()\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    ans=(tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"<|assistant|>\")[1])\n",
    "    print(time.time()-st_time)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.ChatInterface(fn=answer_generate, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"Echo Bot\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuned:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# Hajj started as a pilgrimage to the Kaaba, which was a sacred site for the ancient Arab tribes. The Kaaba was believed to be the house of Allah, and the pilgrimage was a way to honor and worship Him. The practice of Hajj became more formalized and structured over time, with specific rituals and traditions developing. Today, Hajj is a significant religious event for Muslims around the world.\n",
    "# 108.65042996406555\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Hajj is one of the five pillars of Islam, and its history dates back to the time of the Prophet Muhammad. The first recorded Hajj was in 632 CE, during the Prophet's lifetime. The rituals of Hajj have evolved over time, with some changes made by the Prophet himself and others added by subsequent generations of Muslims. The Hajj pilgrimage has also played a significant role in Islamic history, serving as a symbol of unity and solidarity among Muslims from different parts of the world. Today, Hajj remains a central part of Islamic practice, attracting millions of pilgrims each year.\n",
    "# 159.5786316394806\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# Hajj is a religious pilgrimage to Mecca, Saudi Arabia, that is an essential part of Islamic faith. The practice of hajj dates back to the time of the Prophet Muhammad, who established it as a religious obligation for Muslims. The pilgrimage is a symbolic reenactment of the experiences of the Prophet Muhammad and his wife, Khadijah, during their pilgrimage to Mecca. The practice of hajj has been an integral part of Islamic tradition for over 1,400 years.\n",
    "# 134.81562113761902\n",
    "\n",
    "# Base:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# The practice of Hajj, as one of the five pillars of Islam, can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. However, the concept of pilgrimage to the holy city of Mecca for worship and devotion can be traced back to pre-Islamic times, as evidenced by historical records and archaeological findings. The ancient Arab tribes used to visit the Kaaba, a cube-shaped structure in the center of the Grand Mosque in Mecca, as a place of worship and pilgrimage. The Prophet Muhammad's mission to unify and spread Islam brought a new dimension to the practice of Hajj, making it a mandatory religious obligation for all able-bodied Muslims who can afford it to perform once in their lifetime.\n",
    "# 208.76106905937195\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Certainly! The practice of Hajj, which is one of the five pillars of Islam, has a rich and fascinating history that dates back over 1,400 years.\n",
    "\n",
    "# The origins of Hajj can be traced back to the time of the Prophet Muhammad, who received the first revelation of the Quran in 610 CE. According to Islamic tradition, the Prophet Muhammad was commanded by God to make a pilgrimage to the holy city of Mecca, which at the time was a pagan center of worship.\n",
    "\n",
    "# The Prophet Muhammad completed his first pilgrimage, known as the Farewell Pilgrimage, in 632 CE, just a few months before his death. This pilgrimage is considered to be the first official Hajj, and it established many of the rituals and traditions that are still observed today.\n",
    "\n",
    "# Over the centuries, Hajj has played a significant role in the history of Islam and the Muslim world. It has been a source of spiritual renewal and inspiration for countless generations of Muslims, and it has also been a powerful force for unity and solidarity among the faithful.\n",
    "\n",
    "# Throughout history, Hajj\n",
    "# 297.3512797355652\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# The origin of Hajj can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. According to Islamic tradition, Hajj was first performed by Prophet Ibrahim (Abraham) and his son Prophet Ismail (Ishmael) as an act of submission to Allah (God).\n",
    "\n",
    "# However, the annual pilgrimage to the holy city of Mecca, which is an integral part of Hajj, was instituted by Prophet Muhammad during his lifetime. The Prophet's teachings and practices related to Hajj have been preserved and followed by Muslims ever since.\n",
    "\n",
    "# The purpose of Hajj, as stated in the Quran, is to commemorate the unity and brotherhood of the human race, and to affirm the oneness of Allah. It is also a time for Muslims to seek forgiveness, make supplications, and renew their faith and commitment to Allah.\n",
    "\n",
    "# In summary, the origin of Hajj can be traced back to the time of Prophet Ibrahim, but its current form and significance are rooted in the teachings and practices of Prophet Muhammad.\n",
    "# 295.48454308509827\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()//1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.current_device())\n",
    "# additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
