{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sdm/anaconda3/envs/llmtesting/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/bling-1b-0.1\")  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"llmware/bling-1b-0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sdm/Desktop/shakib/KUET LLM/data/rag4_dhiman.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag2V3_2k20.docx',\n",
       " '/home/sdm/Desktop/shakib/KUET LLM/data/rag1_sadia_omar.docx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = r'/home/sdm/Desktop/shakib/KUET LLM/data/'\n",
    "file_pattern = '*.docx'  \n",
    "file_paths = glob.glob(directory_path + file_pattern)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc=[]\n",
    "for x in file_paths:\n",
    "    loader = Docx2txtLoader(x)\n",
    "    documents=loader.load()\n",
    "    # print(((documents)[0]).page_content)\n",
    "    all_doc.append(str(documents[0].page_content))\n",
    "docs='\\n\\n'.join(all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Mar 23, 11:15:36] #> Creating directory .ragatouille/colbert/indexes/bling_colbert \n",
      "\n",
      "\n",
      "[Mar 23, 11:15:38] [0] \t\t #> Encoding 190 passages..\n",
      "[Mar 23, 11:15:39] [0] \t\t avg_doclen_est = 177.55262756347656 \t len(local_sample) = 190\n",
      "[Mar 23, 11:15:39] [0] \t\t Creating 2,048 partitions.\n",
      "[Mar 23, 11:15:39] [0] \t\t *Estimated* 33,734 embeddings.\n",
      "[Mar 23, 11:15:39] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/bling_colbert/plan.json ..\n",
      "used 20 iterations (0.1652s) to cluster 32049 items into 2048 clusters\n",
      "[Mar 23, 11:15:39] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Mar 23, 11:15:40] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.032, 0.032, 0.032, 0.028, 0.029, 0.032, 0.03, 0.031, 0.03, 0.032, 0.031, 0.031, 0.033, 0.031, 0.032, 0.031, 0.031, 0.031, 0.03, 0.032, 0.034, 0.034, 0.031, 0.032, 0.031, 0.031, 0.036, 0.032, 0.031, 0.034, 0.034, 0.034, 0.035, 0.031, 0.033, 0.029, 0.034, 0.031, 0.031, 0.036, 0.034, 0.029, 0.032, 0.03, 0.031, 0.033, 0.032, 0.035, 0.035, 0.03, 0.03, 0.032, 0.033, 0.032, 0.031, 0.034, 0.034, 0.032, 0.038, 0.03, 0.03, 0.033, 0.031, 0.033, 0.034, 0.033, 0.032, 0.033, 0.031, 0.032, 0.032, 0.029, 0.031, 0.032, 0.032, 0.03, 0.034, 0.033, 0.031, 0.033, 0.034, 0.032, 0.033, 0.034, 0.033, 0.032, 0.033, 0.033, 0.032, 0.037, 0.033, 0.035, 0.033, 0.033, 0.033, 0.033, 0.038, 0.03, 0.033, 0.033, 0.032, 0.034, 0.032, 0.03, 0.033, 0.029, 0.031, 0.033, 0.03, 0.032, 0.033, 0.034, 0.035, 0.03, 0.032, 0.032, 0.035, 0.034, 0.031, 0.034, 0.031, 0.03, 0.033, 0.033, 0.031, 0.032, 0.033, 0.029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 23, 11:15:41] [0] \t\t #> Encoding 190 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3518.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 23, 11:15:41] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Mar 23, 11:15:41] #> Building the emb2pid mapping..\n",
      "[Mar 23, 11:15:41] len(emb2pid) = 33735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 150610.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 23, 11:15:41] #> Saved optimized IVF to .ragatouille/colbert/indexes/bling_colbert/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/bling_colbert'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=[docs],\n",
    "    index_name=\"bling_colbert\",\n",
    "    max_document_length=256,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# template='''\n",
    "# <human>: \" {context} \"\\n\" {question} \"\\n\" + \"<bot>:\n",
    "# '''\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 100,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:57,  8.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "dat=[]\n",
    "cnt=0\n",
    "df=pd.read_excel(r\"data/KUET information2k20.xlsx\")\n",
    "df.head()\n",
    "for id,ques in tqdm.tqdm(zip(df['id'],df['Question'])):\n",
    "    ans=rag_chain.invoke(ques)\n",
    "    dat.append({\n",
    "        \"id\":id,\n",
    "        \"question\":ques,\n",
    "        \"answer\":ans\n",
    "    })\n",
    "    if cnt==20:\n",
    "        break\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa=pd.DataFrame(dat)\n",
    "model_ans=\"bling_rag_without_finetuneV2\"\n",
    "sa.to_excel(f\"data//{model_ans}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
